{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e167b149",
   "metadata": {},
   "source": [
    "# SciBERT Multi-label Classification with Global Context\n",
    "\n",
    "This notebook demonstrates how to train a multi-label classification model using SciBERT and global context features. The process includes data loading, preprocessing, tokenization, model setup, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1c160",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We start by importing all necessary libraries for data handling, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515eed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e2d1c",
   "metadata": {},
   "source": [
    "## Device Setup\n",
    "\n",
    "Check if a GPU is available and set the device for PyTorch accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0  # Select the first GPU\n",
    "    torch.cuda.set_device(device_id)\n",
    "    print(f\"Using GPU: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd1cae",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "Load the ACT2 dataset, select relevant columns, and rename the label column for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and rename label column\n",
    "df_act2_full = pd.read_csv('ACT2_dataset.tsv', sep='\\t', usecols=['cited_title','cited_abstract','citation_context', 'unique_id','citation_class_label'])\n",
    "df_act2_full.rename(columns={'citation_class_label': 'labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce1894",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Fill missing values in the title and abstract columns with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df_act2_full['cited_title'] = df_act2_full['cited_title'].fillna('')\n",
    "df_act2_full['cited_abstract'] = df_act2_full['cited_abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ae5b3",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Concatenate the title, abstract, and citation context into a single input string for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input string for the model\n",
    "df_act2_full['input_model'] = df_act2_full['cited_title'] + \" \" + df_act2_full['cited_abstract'] +  \" [ES_SEP] \" + df_act2_full['citation_context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00b1d4",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "Split the dataset into test and train sets. The first 1000 samples are used for testing, and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "df_act2_test = df_act2_full.head(1000)\n",
    "df_act2_train = df_act2_full.tail(len(df_act2_full) - 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0ed6b",
   "metadata": {},
   "source": [
    "## Train/Validation Split\n",
    "\n",
    "Further split the training data into train and validation sets, stratified by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b010ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split for train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    df_act2_train,\n",
    "    test_size=0.2,\n",
    "    stratify=df_act2_train['labels'],\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Train data size: {len(train_df)}\")\n",
    "print(f\"Validation data size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31d1dd",
   "metadata": {},
   "source": [
    "## Convert to HuggingFace Datasets\n",
    "\n",
    "Convert the pandas DataFrames to HuggingFace Dataset objects for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    'validation': Dataset.from_pandas(val_df, preserve_index=False),\n",
    "    'test': Dataset.from_pandas(df_act2_test, preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66fd7e",
   "metadata": {},
   "source": [
    "## Tokenizer Setup\n",
    "\n",
    "Initialize the SciBERT tokenizer and add custom tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and add custom tokens\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_tokens([\"[ES_SEP]\", \"#CITATION_TAG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af0137",
   "metadata": {},
   "source": [
    "## Tokenization Example\n",
    "\n",
    "Tokenize a sample input to verify the tokenizer setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sample input\n",
    "batch = train_df.iloc[0]\n",
    "text = batch[\"input_model\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee65090",
   "metadata": {},
   "source": [
    "## Tokenize the Dataset\n",
    "\n",
    "Define a tokenization function and apply it to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c503d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"input_model\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize all splits\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d5d2d",
   "metadata": {},
   "source": [
    "## Label Mapping\n",
    "\n",
    "Define the mapping between label IDs and label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mappings\n",
    "id2label = {\n",
    "    0 : \"Background\",\n",
    "    1: \"Compares_contrasts\",\n",
    "    2: \"Extension\",\n",
    "    3: \"Future\",\n",
    "    4: \"Motivation\",\n",
    "    5: \"Uses\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbebac5",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Load the SciBERT model for sequence classification and resize the token embeddings to include new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and resize embeddings\n",
    "num_labels = len(id2label)\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54dcd3",
   "metadata": {},
   "source": [
    "## Training Arguments\n",
    "\n",
    "Set up the training arguments, including batch size, learning rate, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e340953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "batch_size = 32\n",
    "training_dir = \"./checkpoints/scibert_training_global_info\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs/global_train\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac425916",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Set up early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387eb58",
   "metadata": {},
   "source": [
    "## Metrics Function\n",
    "\n",
    "Define a function to compute accuracy and macro F1 score during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc771d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\n",
    "        \"f1\": f1_score,\n",
    "        \"accuracy\": acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fe2fc",
   "metadata": {},
   "source": [
    "## Trainer Setup\n",
    "\n",
    "Initialize the HuggingFace Trainer with the model, data, metrics, and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_citation_env",
   "language": "python",
   "name": "local_citation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
